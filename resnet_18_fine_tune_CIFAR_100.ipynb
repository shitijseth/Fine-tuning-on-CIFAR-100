{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPInj-ExYZ7y",
        "outputId": "a0303188-4aaf-44d7-8bcb-24e1ae8f0f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import logging\n",
        "from torchvision.datasets import CIFAR100\n",
        "import numpy as np\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def prepare_cifar100_dataset(output_dir: str, num_classes: int = 5, samples_per_class: int = 10):\n",
        "    \"\"\"\n",
        "    Prepares a subset of CIFAR100 dataset for LLAVA fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        output_dir: Directory to save the processed dataset\n",
        "        num_classes: Number of classes to include\n",
        "        samples_per_class: Number of samples per class\n",
        "    \"\"\"\n",
        "    # Create output directories\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    image_dir = os.path.join(output_dir, \"images\")\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "    # Download CIFAR100\n",
        "    logger.info(\"Downloading CIFAR100 dataset...\")\n",
        "    dataset = CIFAR100(root='./data', train=True, download=True)\n",
        "\n",
        "    # Get class names\n",
        "    class_names = dataset.classes[:num_classes]\n",
        "\n",
        "    # Initialize annotations dictionary\n",
        "    annotations = {}\n",
        "\n",
        "    # Track indices for each class\n",
        "    class_indices = {i: [] for i in range(num_classes)}\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        if label < num_classes:\n",
        "            class_indices[label].append(idx)\n",
        "\n",
        "    # Process each class\n",
        "    for class_idx, class_name in enumerate(class_names):\n",
        "        # Get random samples for this class\n",
        "        selected_indices = np.random.choice(\n",
        "            class_indices[class_idx],\n",
        "            size=min(samples_per_class, len(class_indices[class_idx])),\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        for sample_idx, idx in enumerate(selected_indices):\n",
        "            image, _ = dataset[idx]\n",
        "\n",
        "            # Create filename\n",
        "            image_filename = f\"{class_name}_{sample_idx}.png\"\n",
        "            image_path = os.path.join(image_dir, image_filename)\n",
        "\n",
        "            # Save image\n",
        "            image.save(image_path)\n",
        "\n",
        "            # Create annotation\n",
        "            annotations[image_filename] = {\n",
        "                \"category\": class_name,\n",
        "                \"conversations\": [\n",
        "                    {\n",
        "                        \"from\": \"human\",\n",
        "                        \"value\": \"What category does this image belong to?\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"from\": \"assistant\",\n",
        "                        \"value\": f\"This image belongs to category {class_name}.\"\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "\n",
        "        logger.info(f\"Processed {samples_per_class} images for class {class_name}\")\n",
        "\n",
        "    # Save annotations\n",
        "    annotations_path = os.path.join(output_dir, \"annotations.json\")\n",
        "    with open(annotations_path, 'w') as f:\n",
        "        json.dump(annotations, f, indent=2)\n",
        "\n",
        "    logger.info(f\"Dataset created at {output_dir}\")\n",
        "    logger.info(f\"Total classes: {len(class_names)}\")\n",
        "    logger.info(f\"Classes included: {', '.join(class_names)}\")\n",
        "    logger.info(f\"Total images: {len(annotations)}\")\n",
        "\n",
        "    return image_dir, annotations_path\n",
        "\n",
        "def verify_dataset(image_dir: str, annotations_path: str):\n",
        "    \"\"\"\n",
        "    Verify that all images in the annotations exist and can be opened.\n",
        "    \"\"\"\n",
        "    with open(annotations_path, 'r') as f:\n",
        "        annotations = json.load(f)\n",
        "\n",
        "    all_valid = True\n",
        "    for image_filename in annotations.keys():\n",
        "        image_path = os.path.join(image_dir, image_filename)\n",
        "        if not os.path.exists(image_path):\n",
        "            logger.error(f\"Missing image: {image_filename}\")\n",
        "            all_valid = False\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with Image.open(image_path) as img:\n",
        "                img.verify()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Invalid image {image_filename}: {str(e)}\")\n",
        "            all_valid = False\n",
        "\n",
        "    return all_valid\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Create dataset\n",
        "    output_dir = \"cifar100_llava_dataset\"\n",
        "    image_dir, annotations_path = prepare_cifar100_dataset(\n",
        "        output_dir,\n",
        "        num_classes=5,  # Using 5 classes\n",
        "        samples_per_class=10  # 10 images per class\n",
        "    )\n",
        "\n",
        "    # Verify dataset\n",
        "    if verify_dataset(image_dir, annotations_path):\n",
        "        logger.info(\"Dataset verified successfully!\")\n",
        "    else:\n",
        "        logger.error(\"Dataset verification failed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_EOIV-daLcN",
        "outputId": "9994be85-7221-43c0-f18c-bb01d588afd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "K8SAhELyZ4xT",
        "outputId": "752d7cda-9109-46e3-f200-4cca9bbeccc7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Error downloading model: HTTP Error 404: Not Found\n",
            "ERROR:__main__:\n",
            "            Please download manually from:\n",
            "            https://huggingface.co/mys/ggml_llava-v1.5-7b/tree/main\n",
            "            \n",
            "ERROR:__main__:An error occurred: HTTP Error 404: Not Found\n"
          ]
        },
        {
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a8bd4ac8d46b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-a8bd4ac8d46b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Download/verify model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_model_downloaded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Initialize evaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-a8bd4ac8d46b>\u001b[0m in \u001b[0;36mensure_model_downloaded\u001b[0;34m(model_dir)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mmodel_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://huggingface.co/mys/ggml_llava-v1.5-7b/tree/main/ggml-model-q4_k.gguf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model downloaded successfully to {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-a8bd4ac8d46b>\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(url, filepath)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mDownload\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mprogress\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtotal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content-length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mblock_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m  \u001b[0;31m# 1MB chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import logging\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor\n",
        "from llama_cpp import Llama\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import urllib.request\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def download_file(url, filepath):\n",
        "    \"\"\"\n",
        "    Download a file with progress bar\n",
        "    \"\"\"\n",
        "    response = urllib.request.urlopen(url)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024 * 1024  # 1MB chunks\n",
        "\n",
        "    with open(filepath, 'wb') as f, tqdm(\n",
        "        total=total_size,\n",
        "        unit='iB',\n",
        "        unit_scale=True,\n",
        "        desc=f\"Downloading {os.path.basename(filepath)}\"\n",
        "    ) as pbar:\n",
        "        while True:\n",
        "            buffer = response.read(block_size)\n",
        "            if not buffer:\n",
        "                break\n",
        "            f.write(buffer)\n",
        "            pbar.update(len(buffer))\n",
        "\n",
        "def ensure_model_downloaded(model_dir=\"models\"):\n",
        "    \"\"\"\n",
        "    Ensure the GGUF model is downloaded\n",
        "    \"\"\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    model_path = os.path.join(model_dir, \"llava-v1.5-7b-q4_k.gguf\")\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        logger.info(\"Model not found. Downloading...\")\n",
        "        model_url = \"https://huggingface.co/mys/ggml_llava-v1.5-7b/tree/main/ggml-model-q4_k.gguf\"\n",
        "        try:\n",
        "            download_file(model_url, model_path)\n",
        "            logger.info(f\"Model downloaded successfully to {model_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error downloading model: {str(e)}\")\n",
        "            logger.error(\"\"\"\n",
        "            Please download manually from:\n",
        "            https://huggingface.co/mys/ggml_llava-v1.5-7b/tree/main\n",
        "            \"\"\")\n",
        "            raise\n",
        "    else:\n",
        "        logger.info(f\"Model already exists at {model_path}\")\n",
        "\n",
        "    return model_path\n",
        "\n",
        "class LlavaGGUFEvaluator:\n",
        "    def __init__(self, model_path, n_gpu_layers=20, n_ctx=2048):\n",
        "        \"\"\"\n",
        "        Initialize LLAVA GGUF model\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading model from {model_path}\")\n",
        "        self.model = Llama(\n",
        "            model_path=model_path,\n",
        "            n_gpu_layers=n_gpu_layers,\n",
        "            n_ctx=n_ctx,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        self.processor = AutoProcessor.from_pretrained(\"liuhaotian/llava-v1.5-7b\")\n",
        "\n",
        "    def predict_single(self, image_path):\n",
        "        \"\"\"Predict category for a single image\"\"\"\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image_tensor = self.processor.image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "\n",
        "        prompt = \"What category does this image belong to?\"\n",
        "\n",
        "        response = self.model.create_chat_completion(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": prompt},\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\"url\": image_path}}\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=100,\n",
        "            temperature=0.0\n",
        "        )\n",
        "\n",
        "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    def evaluate_dataset(self, image_dir, annotations_path):\n",
        "        \"\"\"Evaluate the model on entire dataset\"\"\"\n",
        "        with open(annotations_path, 'r') as f:\n",
        "            annotations = json.load(f)\n",
        "\n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "\n",
        "        logger.info(\"Starting evaluation...\")\n",
        "        for image_file, anno in tqdm(annotations.items()):\n",
        "            image_path = os.path.join(image_dir, image_file)\n",
        "            true_category = anno['category']\n",
        "\n",
        "            prediction = self.predict_single(image_path)\n",
        "\n",
        "            try:\n",
        "                pred_category = prediction.split(\"category \")[-1].rstrip(\".\")\n",
        "            except:\n",
        "                pred_category = \"unknown\"\n",
        "\n",
        "            true_labels.append(true_category)\n",
        "            pred_labels.append(pred_category)\n",
        "\n",
        "            logger.debug(f\"True: {true_category}, Predicted: {pred_category}\")\n",
        "\n",
        "        accuracy = accuracy_score(true_labels, pred_labels)\n",
        "        report = classification_report(true_labels, pred_labels)\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'classification_report': report,\n",
        "            'predictions': list(zip(true_labels, pred_labels))\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    # Paths\n",
        "    dataset_dir = \"cifar100_llava_dataset\"\n",
        "    image_dir = os.path.join(dataset_dir, \"images\")\n",
        "    annotations_path = os.path.join(dataset_dir, \"annotations.json\")\n",
        "\n",
        "    try:\n",
        "        # Download/verify model\n",
        "        model_path = ensure_model_downloaded()\n",
        "\n",
        "        # Initialize evaluator\n",
        "        evaluator = LlavaGGUFEvaluator(\n",
        "            model_path=model_path,\n",
        "            n_gpu_layers=20  # Reduce this if you run into memory issues\n",
        "        )\n",
        "\n",
        "        # Run evaluation\n",
        "        logger.info(\"Starting evaluation...\")\n",
        "        results = evaluator.evaluate_dataset(image_dir, annotations_path)\n",
        "\n",
        "        # Print results\n",
        "        logger.info(\"\\nEvaluation Results:\")\n",
        "        logger.info(f\"Accuracy: {results['accuracy']:.4f}\")\n",
        "        logger.info(\"\\nClassification Report:\")\n",
        "        logger.info(results['classification_report'])\n",
        "\n",
        "        # Save results\n",
        "        with open('evaluation_results.json', 'w') as f:\n",
        "            json.dump({\n",
        "                'accuracy': results['accuracy'],\n",
        "                'predictions': results['predictions']\n",
        "            }, f, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqeOJig9nscm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLG-xqWynsZ_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdl9khKVnsXa",
        "outputId": "50cc652d-27a7-451f-97ac-5d6413d29a8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Baseline Test Accuracy (Before Fine-Tuning): 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: quantized::linear_dynamic: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 | Loss: 11.0298 | Test Accuracy: 0.00%\n",
            "Epoch 2/3 | Loss: 11.0365 | Test Accuracy: 0.00%\n",
            "Epoch 3/3 | Loss: 11.0374 | Test Accuracy: 0.00%\n",
            "\n",
            "Final Test Accuracy after Fine-Tuning: 0.00%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory ./llava_finetuned_cifar100 does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-01f5d6662d5c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./llava_finetuned_cifar100\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/pytorch_model_quantized.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Quantized model and tokenizer saved.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             _save(\n\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ./llava_finetuned_cifar100 does not exist."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.quantization\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "###############################################\n",
        "# 1. Define a Simple Multimodal LLaVA‑like Model\n",
        "###############################################\n",
        "class SimpleLLaVA(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(SimpleLLaVA, self).__init__()\n",
        "        # Use a pre‑trained ResNet‑18 as the vision encoder.\n",
        "        self.vision_encoder = models.resnet18(pretrained=True)\n",
        "        # Remove the final classification layer.\n",
        "        self.vision_encoder.fc = nn.Identity()\n",
        "        # Project the ResNet output (512‑dim) to our embedding dimension.\n",
        "        self.vision_projection = nn.Linear(512, embed_dim)\n",
        "\n",
        "        # Text embedding layer.\n",
        "        self.text_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # A small transformer decoder: 2 layers with 8 heads each.\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=8)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n",
        "\n",
        "        # Language modeling head that maps transformer outputs to vocabulary logits.\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images, labels=None):\n",
        "        # Encode the image.\n",
        "        vision_features = self.vision_encoder(images)  # [batch, 512]\n",
        "        vision_embeds = self.vision_projection(vision_features).unsqueeze(1)  # [batch, 1, embed_dim]\n",
        "\n",
        "        # Embed the combined text (prompt + target).\n",
        "        text_embeds = self.text_embedding(input_ids)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Prepend the vision embedding to the text embeddings.\n",
        "        decoder_input = torch.cat([vision_embeds, text_embeds], dim=1)  # [batch, 1+seq_len, embed_dim]\n",
        "\n",
        "        # Use the vision embedding (transposed) as memory for the decoder.\n",
        "        memory = vision_embeds.transpose(0, 1)  # [1, batch, embed_dim]\n",
        "\n",
        "        # Transformer expects (seq_len, batch, embed_dim)\n",
        "        decoder_input = decoder_input.transpose(0, 1)  # [1+seq_len, batch, embed_dim]\n",
        "        seq_len_total = decoder_input.size(0)\n",
        "        # Create a causal mask.\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len_total).to(decoder_input.device)\n",
        "\n",
        "        # Transformer decoding.\n",
        "        decoder_output = self.transformer_decoder(decoder_input, memory, tgt_mask=tgt_mask)\n",
        "        decoder_output = decoder_output.transpose(0, 1)  # [batch, 1+seq_len, embed_dim]\n",
        "\n",
        "        # Only the text part is used for language modeling (skip the vision token).\n",
        "        lm_logits = self.lm_head(decoder_output[:, 1:, :])  # [batch, seq_len, vocab_size]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Compute cross-entropy loss over text tokens; assume labels shape [batch, seq_len] (prompt tokens are masked as -100)\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(lm_logits.reshape(-1, lm_logits.size(-1)), labels.reshape(-1))\n",
        "        return {\"loss\": loss, \"logits\": lm_logits}\n",
        "\n",
        "###############################################\n",
        "# 2. Setup Tokenizer and Prepare CIFAR‑100 Data\n",
        "###############################################\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Use GPT‑2’s tokenizer for demonstration.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# GPT‑2 does not have a pad token; set it to its eos_token.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define image transforms for CIFAR‑100.\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to suit ResNet input.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Download CIFAR‑100 datasets.\n",
        "train_cifar = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_cifar  = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Custom dataset: for each sample, build a prompt and a target answer.\n",
        "class CIFAR100LLAVADataset(Dataset):\n",
        "    def __init__(self, cifar_dataset, tokenizer):\n",
        "        self.cifar_dataset = cifar_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cifar_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.cifar_dataset[idx]\n",
        "        class_name = self.cifar_dataset.classes[label]\n",
        "        prompt = \"Please classify the following image:\"\n",
        "        target = f\" Answer: {class_name}\"\n",
        "        return {\"image\": image, \"prompt\": prompt, \"target\": target}\n",
        "\n",
        "# Collate function: combines image and text data.\n",
        "def collate_fn(batch):\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "    prompts = [item[\"prompt\"] for item in batch]\n",
        "    targets_str = [item[\"target\"] for item in batch]  # Keep target strings for evaluation.\n",
        "\n",
        "    # Tokenize prompts.\n",
        "    prompt_encodings = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
        "    # Tokenize target answers.\n",
        "    target_encodings = tokenizer(targets_str, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Combine input_ids: prompt tokens followed by target tokens.\n",
        "    input_ids = torch.cat([prompt_encodings.input_ids, target_encodings.input_ids], dim=1)\n",
        "    # Combine attention masks.\n",
        "    attention_mask = torch.cat([prompt_encodings.attention_mask, target_encodings.attention_mask], dim=1)\n",
        "\n",
        "    # Prepare labels: mask out prompt tokens (-100) and keep target tokens.\n",
        "    prompt_len = prompt_encodings.input_ids.size(1)\n",
        "    labels_prompt = torch.full(prompt_encodings.input_ids.shape, -100)\n",
        "    labels = torch.cat([labels_prompt, target_encodings.input_ids], dim=1)\n",
        "\n",
        "    return {\n",
        "        \"images\": images,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"targets_str\": targets_str,  # For evaluation.\n",
        "        \"prompt_len\": prompt_len     # For evaluation.\n",
        "    }\n",
        "\n",
        "# Create dataset instances.\n",
        "train_dataset = CIFAR100LLAVADataset(train_cifar, tokenizer)\n",
        "test_dataset  = CIFAR100LLAVADataset(test_cifar, tokenizer)\n",
        "\n",
        "# Create DataLoaders.\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "###############################################\n",
        "# 3. Initialize Model, Apply Dynamic Quantization, and Set Optimizer\n",
        "###############################################\n",
        "vocab_size = tokenizer.vocab_size\n",
        "embed_dim = 256  # Chosen embedding dimension.\n",
        "model = SimpleLLaVA(vocab_size, embed_dim).to(device)\n",
        "\n",
        "# Apply dynamic quantization to all nn.Linear layers.\n",
        "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "optimizer = optim.Adam(quantized_model.parameters(), lr=1e-5)\n",
        "num_epochs = 3\n",
        "\n",
        "###############################################\n",
        "# 4. Define Evaluation Function\n",
        "###############################################\n",
        "def evaluate_model(model, data_loader, tokenizer, device):\n",
        "    model.eval()\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            images = batch[\"images\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            targets_str = batch[\"targets_str\"]  # These are strings.\n",
        "            prompt_len = batch[\"prompt_len\"]\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                images=images,\n",
        "                labels=None\n",
        "            )\n",
        "            logits = outputs[\"logits\"]  # [batch, seq_len, vocab_size]\n",
        "            predicted_ids = logits.argmax(dim=-1)  # [batch, seq_len]\n",
        "            # Only consider tokens corresponding to target answer (after the prompt).\n",
        "            predicted_target_ids = predicted_ids[:, prompt_len:]\n",
        "\n",
        "            for pred_ids, target in zip(predicted_target_ids, targets_str):\n",
        "                pred_text = tokenizer.decode(pred_ids, skip_special_tokens=True).strip().lower()\n",
        "                target_text = target.strip().lower()\n",
        "                if target_text in pred_text:\n",
        "                    correct_predictions += 1\n",
        "                total_samples += 1\n",
        "    accuracy = (correct_predictions / total_samples * 100) if total_samples > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "###############################################\n",
        "# 5. Baseline Evaluation (Before Fine‑Tuning)\n",
        "###############################################\n",
        "baseline_accuracy = evaluate_model(quantized_model, test_loader, tokenizer, device)\n",
        "print(f\"Baseline Test Accuracy (Before Fine-Tuning): {baseline_accuracy:.2f}%\")\n",
        "\n",
        "###############################################\n",
        "# 6. Fine‑Tuning Loop with Performance Printing During Training\n",
        "###############################################\n",
        "quantized_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        images = batch[\"images\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = quantized_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            images=images,\n",
        "            labels=labels\n",
        "        )\n",
        "        loss = outputs[\"loss\"]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    # Evaluate on the test set at the end of each epoch.\n",
        "    test_accuracy = evaluate_model(quantized_model, test_loader, tokenizer, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "###############################################\n",
        "# 7. Final Evaluation and Save the Quantized Model and Tokenizer\n",
        "###############################################\n",
        "final_accuracy = evaluate_model(quantized_model, test_loader, tokenizer, device)\n",
        "print(f\"\\nFinal Test Accuracy after Fine-Tuning: {final_accuracy:.2f}%\")\n",
        "\n",
        "model_path = \"./llava_finetuned_cifar100\"\n",
        "torch.save(quantized_model.state_dict(), model_path + \"/pytorch_model_quantized.bin\")\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(\"Quantized model and tokenizer saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V59aORWGnsUz"
      },
      "outputs": [],
      "source": [
        "###############################################\n",
        "# 6. Evaluation: Show Performance on Test Set\n",
        "###############################################\n",
        "model.eval()\n",
        "total_samples = 0\n",
        "correct_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        images = batch[\"images\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        targets_str = batch[\"targets_str\"]  # list of target answer strings\n",
        "        prompt_len = batch[\"prompt_len\"]     # length of the prompt tokens\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            images=images,\n",
        "            labels=None  # No labels for inference.\n",
        "        )\n",
        "        # Get logits and perform greedy decoding.\n",
        "        logits = outputs[\"logits\"]  # [batch, seq_len, vocab_size]\n",
        "        predicted_ids = logits.argmax(dim=-1)  # [batch, seq_len]\n",
        "        # We only care about tokens corresponding to the target answer (after the prompt).\n",
        "        predicted_target_ids = predicted_ids[:, prompt_len:]\n",
        "\n",
        "        for pred_ids, target in zip(predicted_target_ids, targets_str):\n",
        "            pred_text = tokenizer.decode(pred_ids, skip_special_tokens=True).strip()\n",
        "            # A simple match: check if the target answer is contained in the predicted text.\n",
        "            if target.strip().lower() in pred_text.lower():\n",
        "                correct_predictions += 1\n",
        "            total_samples += 1\n",
        "\n",
        "accuracy = correct_predictions / total_samples * 100\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0j88fn6nsQA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgw4AboJnsHK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "###############################################\n",
        "# 1. Define a Simple Multimodal LLaVA‑like Model\n",
        "###############################################\n",
        "class SimpleLLaVA(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(SimpleLLaVA, self).__init__()\n",
        "        # Use a pre‑trained ResNet‑18 as the vision encoder.\n",
        "        self.vision_encoder = models.resnet18(pretrained=True)\n",
        "        # Remove the final classification layer.\n",
        "        self.vision_encoder.fc = nn.Identity()\n",
        "        # Project the ResNet output (512‑dim) to our embedding dimension.\n",
        "        self.vision_projection = nn.Linear(512, embed_dim)\n",
        "\n",
        "        # Text embedding layer.\n",
        "        self.text_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # A small transformer decoder: 2 layers with 8 heads each.\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=8)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n",
        "\n",
        "        # Language modeling head that maps transformer outputs to vocabulary logits.\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images, labels=None):\n",
        "        # Encode the image.\n",
        "        vision_features = self.vision_encoder(images)  # [batch, 512]\n",
        "        vision_embeds = self.vision_projection(vision_features).unsqueeze(1)  # [batch, 1, embed_dim]\n",
        "\n",
        "        # Embed the combined text (prompt + target).\n",
        "        text_embeds = self.text_embedding(input_ids)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Prepend the vision embedding to the text embeddings.\n",
        "        decoder_input = torch.cat([vision_embeds, text_embeds], dim=1)  # [batch, 1+seq_len, embed_dim]\n",
        "\n",
        "        # Use the vision embedding (transposed) as memory for the decoder.\n",
        "        memory = vision_embeds.transpose(0, 1)  # [1, batch, embed_dim]\n",
        "\n",
        "        # Transformer expects inputs in shape (seq_len, batch, embed_dim)\n",
        "        decoder_input = decoder_input.transpose(0, 1)  # [1+seq_len, batch, embed_dim]\n",
        "        seq_len_total = decoder_input.size(0)\n",
        "        # Create a causal mask.\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len_total).to(decoder_input.device)\n",
        "\n",
        "        # Transformer decoding.\n",
        "        decoder_output = self.transformer_decoder(decoder_input, memory, tgt_mask=tgt_mask)\n",
        "        decoder_output = decoder_output.transpose(0, 1)  # [batch, 1+seq_len, embed_dim]\n",
        "\n",
        "        # Only the text part is used for language modeling (skip the vision token).\n",
        "        lm_logits = self.lm_head(decoder_output[:, 1:, :])  # [batch, seq_len, vocab_size]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Compute cross-entropy loss over text tokens; assume labels shape [batch, seq_len] (with prompt tokens masked as -100)\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(lm_logits.reshape(-1, lm_logits.size(-1)), labels.reshape(-1))\n",
        "        return {\"loss\": loss, \"logits\": lm_logits}\n",
        "\n",
        "###############################################\n",
        "# 2. Setup Tokenizer and Prepare CIFAR‑100 Data\n",
        "###############################################\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Use GPT‑2’s tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# Set pad token to eos_token since GPT‑2 lacks a dedicated pad token.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define image transforms for CIFAR‑100.\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to suit ResNet input.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Download CIFAR‑100 datasets.\n",
        "train_cifar = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_cifar  = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Custom dataset: build a prompt and target answer for each sample.\n",
        "class CIFAR100LLAVADataset(Dataset):\n",
        "    def __init__(self, cifar_dataset, tokenizer):\n",
        "        self.cifar_dataset = cifar_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cifar_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.cifar_dataset[idx]\n",
        "        class_name = self.cifar_dataset.classes[label]\n",
        "        prompt = \"Please classify the following image:\"\n",
        "        target = f\" Answer: {class_name}\"\n",
        "        return {\"image\": image, \"prompt\": prompt, \"target\": target}\n",
        "\n",
        "# Collate function to combine image and text data.\n",
        "def collate_fn(batch):\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "    prompts = [item[\"prompt\"] for item in batch]\n",
        "    targets_str = [item[\"target\"] for item in batch]  # Keep as strings for evaluation.\n",
        "\n",
        "    # Tokenize prompts.\n",
        "    prompt_encodings = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
        "    # Tokenize target answers.\n",
        "    target_encodings = tokenizer(targets_str, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Concatenate prompt and target tokens.\n",
        "    input_ids = torch.cat([prompt_encodings.input_ids, target_encodings.input_ids], dim=1)\n",
        "    attention_mask = torch.cat([prompt_encodings.attention_mask, target_encodings.attention_mask], dim=1)\n",
        "\n",
        "    # Create labels: mask out prompt tokens (set to -100) and keep target tokens.\n",
        "    prompt_len = prompt_encodings.input_ids.size(1)\n",
        "    labels_prompt = torch.full(prompt_encodings.input_ids.shape, -100)\n",
        "    labels = torch.cat([labels_prompt, target_encodings.input_ids], dim=1)\n",
        "\n",
        "    return {\n",
        "        \"images\": images,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"targets_str\": targets_str,  # For evaluation.\n",
        "        \"prompt_len\": prompt_len     # For evaluation.\n",
        "    }\n",
        "\n",
        "# Create dataset instances.\n",
        "train_dataset = CIFAR100LLAVADataset(train_cifar, tokenizer)\n",
        "test_dataset  = CIFAR100LLAVADataset(test_cifar, tokenizer)\n",
        "\n",
        "# Create DataLoaders.\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "###############################################\n",
        "# 3. Initialize Full-Precision Model and Set Optimizer\n",
        "###############################################\n",
        "vocab_size = tokenizer.vocab_size\n",
        "embed_dim = 256\n",
        "model = SimpleLLaVA(vocab_size, embed_dim).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "num_epochs = 3\n",
        "\n",
        "###############################################\n",
        "# 4. Define Evaluation Function\n",
        "###############################################\n",
        "def evaluate_model(model, data_loader, tokenizer, device):\n",
        "    model.eval()\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            images = batch[\"images\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            targets_str = batch[\"targets_str\"]  # Strings.\n",
        "            prompt_len = batch[\"prompt_len\"]\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                images=images,\n",
        "                labels=None\n",
        "            )\n",
        "            logits = outputs[\"logits\"]  # Shape: [batch, seq_len, vocab_size]\n",
        "            predicted_ids = logits.argmax(dim=-1)  # [batch, seq_len]\n",
        "            predicted_target_ids = predicted_ids[:, prompt_len:]  # Only target tokens\n",
        "\n",
        "            for pred_ids, target in zip(predicted_target_ids, targets_str):\n",
        "                pred_text = tokenizer.decode(pred_ids, skip_special_tokens=True).strip().lower()\n",
        "                target_text = target.strip().lower()\n",
        "                if target_text in pred_text:\n",
        "                    correct_predictions += 1\n",
        "                total_samples += 1\n",
        "    accuracy = (correct_predictions / total_samples * 100) if total_samples > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "###############################################\n",
        "# 5. Baseline Evaluation (Before Fine‑Tuning)\n",
        "###############################################\n",
        "baseline_accuracy = evaluate_model(model, test_loader, tokenizer, device)\n",
        "print(f\"Baseline Test Accuracy (Before Fine-Tuning): {baseline_accuracy:.2f}%\")\n",
        "\n",
        "###############################################\n",
        "# 6. Fine‑Tuning Loop with Performance Printing During Training\n",
        "###############################################\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        images = batch[\"images\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            images=images,\n",
        "            labels=labels\n",
        "        )\n",
        "        loss = outputs[\"loss\"]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    # Evaluate after each epoch.\n",
        "    test_accuracy = evaluate_model(model, test_loader, tokenizer, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "###############################################\n",
        "# 7. Final Evaluation, Apply Dynamic Quantization for Inference, and Save Model\n",
        "###############################################\n",
        "final_accuracy = evaluate_model(model, test_loader, tokenizer, device)\n",
        "print(f\"\\nFinal Test Accuracy after Fine-Tuning (Full Precision): {final_accuracy:.2f}%\")\n",
        "\n",
        "# Apply dynamic quantization for inference only.\n",
        "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
        "quantized_accuracy = evaluate_model(quantized_model, test_loader, tokenizer, device)\n",
        "print(f\"Final Test Accuracy after Quantization (Inference): {quantized_accuracy:.2f}%\")\n",
        "\n",
        "# Save the quantized model and tokenizer.\n",
        "model_path = \"./llava_finetuned_cifar100\"\n",
        "torch.save(quantized_model.state_dict(), model_path + \"/pytorch_model_quantized.bin\")\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(\"Quantized model and tokenizer saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}